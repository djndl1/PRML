#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{ifxetex}
\usepackage{ifluatex}\usepackage{fixltx2e}% provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
\else
    \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref

\urlstyle{same}  % don't use monospace font for urls
\usepackage{grffile}
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}

% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp

\def\fps@figure{htbp}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8
\fontencoding T1
\font_roman "lmodern" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "default" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command makeindex
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 2
\use_package esint 1
\use_package mathdots 2
\use_package mathtools 2
\use_package mhchem 2
\use_package stackrel 2
\use_package stmaryrd 2
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth -2
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
1.
 Introduction 
\end_layout

\begin_layout Standard
The problem of searching for patterns in data is a fundamental one and has
 a long and successful history.
 The field of pattern recognition is concerned with the automatic discovery
 of regularities in data through the use of computer algorithms and with
 the use of these regularities to take actions such as classifying the data
 into different categories.
\end_layout

\begin_layout Standard
A heuristic technique
\begin_inset CommandInset href
LatexCommand href
name "heuristic technique"
target "https://en.wikipedia.org/wiki/Heuristic"
literal "false"

\end_inset

, often called simply a heuristic, is any approach to problem solving, learning,
 or discovery that employs a practical method not guaranteed to be optimal
 or perfect, but sufficient for the immediate goals.
 The most fundamental heuristic is trial and error, which can be used in
 everything from matching nuts and bolts to finding the values of variables
 in algebra problems.
\end_layout

\begin_layout Standard
training set: 
\begin_inset Formula $\left\{ x_{1},...,x_{N}\right\} $
\end_inset


\end_layout

\begin_layout Standard
target vector: 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Standard
result expressed as a function 
\begin_inset Formula $y(x)$
\end_inset


\end_layout

\begin_layout Standard
learning = training 
\end_layout

\begin_layout Standard
generalization :the albility to categorize correctly new examples that differ
 from those used for training
\end_layout

\begin_layout Standard
preprocession: to make problem easier to solve, to speed up computation
 
\end_layout

\begin_layout Standard
feature extraction 
\end_layout

\begin_layout Standard
dimensionality reduction
\end_layout

\begin_layout Standard
supervised learning 
\end_layout

\begin_layout Standard
classification regression 
\end_layout

\begin_layout Standard
unsupervised learning: input vectors 
\begin_inset Formula $x$
\end_inset

 without any corresponding targht values 
\end_layout

\begin_layout Standard
clustering: to discover groups of similar examples within the data 
\end_layout

\begin_layout Standard
density estimation: to determine the distribution of data within the input
 space 
\end_layout

\begin_layout Standard
visualization: to project the data from a high-dimensional space down to
 two or three dimensions
\end_layout

\begin_layout Standard
reinforment learning: finding suitable actions to take in a given situation
 in order to maximize a reward by a process of trial and error 
\end_layout

\begin_layout Standard
e.g.
 backgammon, a board position and the result of a dice throw as input, a
 strong move as the output, only at the end of the game that the reward
 is achieved.
 
\end_layout

\begin_layout Standard
credit assignment 
\end_layout

\begin_layout Standard
exploration: try out new kinds of actions to see how effective they are
 exploitation: make use of actions that are known to yield a high reward
 there is a trade-off between the above two
\end_layout

\begin_layout Section
1.1 the example of polynomial curve fitting 
\end_layout

\begin_layout Standard
P4.
 problem description 
\end_layout

\begin_layout Standard
Probability theory: provides a framework for expressing such uncertainty
 in a precise and quantitative manner Decision theory: to exploit this probabili
stic representation in order to make predictions that are optimal according
 to appropriate criteria.
\end_layout

\begin_layout Standard
using a polynomial function of the form 
\begin_inset Formula $y(x,\overrightarrow{w})=\sum_{j=0}^{M}\left\{ w_{j}x^{j}\right\} $
\end_inset

 , linear in the unknown parameters, having important properties, thus called
 
\backslash
textit{linear models}.
 The determination of the values of the coefficients can be done by minimizing
 an error function that measures the misfit between the function and the
 training set data points.
 (P5 1.2)
\end_layout

\begin_layout Standard
choosing the order M leads to MODEL COMPARISON/SELECTION 
\end_layout

\begin_layout Standard
over-fitting, RMS error is introduced to compare different sizes of data.
 (P6 1.3)
\end_layout

\begin_layout Standard
A power series expansion of the sine function contains terms of all orders,
 however, in this case, the coefficients don't go as a power series should
 be.
\end_layout

\begin_layout Standard
the over-fitting problem become less severe as the size of the data set
 increases.
 Another way to say this is that the larger the data set, the more complex
 (in other words more flexible) the model that we can afford to fit to the
 data.
 (P8 table 1.1)
\end_layout

\begin_layout Standard
adopting a Bayesian approach to avoid the over-fitting problem (P9) (Sec.
 1.2.6)
\end_layout

\begin_layout Paragraph
Using relatively complex and flexible models
\end_layout

\begin_layout Standard
regularization: adds a penalty term ot the error function to discourage
 the coefficients from reaching large values, a problem that came forth
 in the method above (P1.4), known as shrinkage in statistics; weight decay
 in the context of neural networks.
\end_layout

\begin_layout Section
1.2 Probability Theory
\end_layout

\begin_layout Standard
uncertainty: a key concept in the field of pattern recognition 
\end_layout

\begin_layout Standard
probability theory: a consistent framework for the quantification and manipulati
on of uncertainty
\end_layout

\begin_layout Standard
Some readings 
\end_layout

\begin_layout Standard
https://en.wikipedia.org/wiki/Principle_of_indifference
\end_layout

\begin_layout Standard
https://en.wikipedia.org/wiki/Classical_definition_of_probability
\end_layout

\begin_layout Standard
The probability of an event is the ratio of the number of cases favorable
 to it, to the number of all cases possible when nothing leads us to expect
 that any one of these cases should occur more than any other, which renders
 them, for us, equally possible.
 Criticism
\end_layout

\begin_layout Standard
https://en.wikipedia.org/wiki/Bayesian_statistics
\end_layout

\begin_layout Standard
Bayesian methods are characterized by concepts and procedures as follows:
\end_layout

\begin_layout Standard
The use of random variables, or more generally unknown quantities, to model
 all sources of uncertainty in statistical models including uncertainty
 resulting from lack of information (see also aleatoric and epistemic uncertaint
y).
 The need to determine the prior probability distribution taking into account
 the available (prior) information.
 The sequential use of Bayes' formula: when more data become available,
 calculate the posterior distribution using Bayes' formula; subsequently,
 the posterior distribution becomes the next prior.
 While for the frequentist a hypothesis is a proposition (which must be
 either true or false), so that the frequentist probability of a hypothesis
 is either 0 or 1, in Bayesian statistics the probability that can be assigned
 to a hypothesis can also be in a range from 0 to 1 if the truth value is
 uncertain.
\end_layout

\begin_layout Standard
https://en.wikipedia.org/wiki/Frequentist_probability 
\end_layout

\begin_layout Standard
it defines an event's probability as the limit of its relative frequency
 in a large number of trials.
 In the frequentist interpretation, probabilities are discussed only when
 dealing with well-defined random experiments (or random samples).
 The set of all possible outcomes of a random experiment is called the sample
 space of the experiment.
 An event is defined as a particular subset of the sample space to be considered.
 For any given event, only one of two possibilities may hold: it occurs
 or it does not.
 The relative frequency of occurrence of an event, observed in a number
 of repetitions of the experiment, is a measure of the probability of that
 event.
 This is the core conception of probability in the frequentist interpretation.
 A claim of the frequentist approach is that in the "long run," as the number
 of trials approaches infinity, the relative frequency will converge exactly
 to the true probability 
\end_layout

\begin_layout Standard
As an interpretation, it is not in conflict with the mathematical axiomatization
 of probability theory; rather, it provides guidance for how to apply mathematic
al probability theory to real-world situations.
 It offers distinct guidance in the construction and design of practical
 experiments, especially when contrasted with the Bayesian interpretation.
 As to whether this guidance is useful, or is apt to mis-interpretation,
 has been a source of controversy.
 Particularly when the frequency interpretation of probability is mistakenly
 assumed to be the only possible basis for frequentist inference.
\end_layout

\begin_layout Standard
https://en.wikipedia.org/wiki/Probability_interpretations 
\end_layout

\begin_layout Standard
The terminology of this topic is rather confusing, in part because probabilities
 are studied within a variety of academic fields.
 The word "frequentist" is especially tricky.
 To philosophers it refers to a particular theory of physical probability,
 one that has more or less been abandoned.
 To scientists, on the other hand, "frequentist probability" is just another
 name for physical (or objective) probability.
 Those who promote Bayesian inference view "frequentist statistics" as an
 approach to statistical inference that recognises only physical probabilities.
 Also the word "objective", as applied to probability, sometimes means exactly
 what "physical" means here, but is also used of evidential probabilities
 that are fixed by rational constraints, such as logical and epistemic probabili
ties.
 
\end_layout

\begin_layout Standard
Para 2.
 P21 
\end_layout

\begin_layout Standard
maximum likelihood, error function, bootstrap P23.
 
\end_layout

\begin_layout Standard
the multiplicative inverse of variance - precision $
\backslash
beta$ independent and identically distributed, abre to i.i.d.
 The advantage of taking the log of a likelihood function - to avoid the
 underflowing of the numerical precision of the computer 
\end_layout

\begin_layout Standard
bias and overfittings P28 bias-variance decomposition
\end_layout

\begin_layout Paragraph
Revisiting curve fitting - a new approach
\end_layout

\begin_layout Standard
The sum-of-squares error function has arisen as a consequence of maximizing
 likelyhood under the assumption of a Gaussian noise distribution.
 The result is a probabilistic model, expressed in terms of the predictive
 distribution that gives the probability distribution over t rather than
 simply a point estimate.
 P30 1.64 
\end_layout

\begin_layout Standard
a more bayesian approach: variables that control the distribution of model
 parameter - HYPERPARAMETERS (1.65) - (1.67) MAP - maximum posterior, give
 the distribution of 
\begin_inset Formula $w$
\end_inset

, i.e.
 its parameter 
\begin_inset Formula $\alpha$
\end_inset

 and the distribution of 
\begin_inset Formula $t$
\end_inset

, we have 
\begin_inset Formula $p(w|x,t,\alpha,\beta)\propto p(t|x,w,\beta)p(w|\alpha)$
\end_inset

, which finally leads to an equivalence between MAP and sum-of-squares error
 function with a regularization parameter.
\end_layout

\begin_layout Paragraph
A fully Bayesian approach
\end_layout

\begin_layout Standard
Given data set containing x and t, and a new point x, evaluate the corresponding
 t, 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are already assumed
\end_layout

\begin_layout Section
1.3 Model Selection 
\end_layout

\begin_layout Paragraph
Test and validation
\end_layout

\begin_layout Standard
If data is plentiful, then one approach is simply to use some of the available
 data to train a range of models, or a given model with a range of values
 for its complexity parameters, and then to compare them on independent
 data, sometimes called a validation set, and select the one having the
 best predictive performance.
 If the model design is iterated many times using a limited size data set,
 then some over fitting to the validation data can occur and so it may be
 necessary to keep aside a third test set on which the performance of the
 selected model is finally evaluated.
\end_layout

\begin_layout Standard
While in many applications, not much data is available.
 
\end_layout

\begin_layout Standard
Cross-validation P32-P33 https://en.wikipedia.org/wiki/Cross-validation_%28statist
ics%29 leave-one-out technique drawback: computationally expensive.
 AIC: akaike information criterion, attempting to correct for the bias of
 maximum likelihood
\end_layout

\begin_layout Section
1.4 The Curse of Dimensionality 
\end_layout

\begin_layout Standard
Think about what a general M-order polynomial with D variables would look
 like: the number of coefficients is proportional to $D^M$ The problem of
 the volume of the surface between $r=1$ and $r=1-
\backslash
epsilon$; in spaces of high dimensionality, most of the volume of a sphere
 is concentrated in a thin shell near the surface.
 Gaussian probability density under different dimensionality D Figure 1.23
 P37 
\end_layout

\begin_layout Standard
Dimensionality characteristics of real data 1.
 confined to a region of the space having lower effective dimensionality
 2.
 having smoothness properties so that small changes in the input variables
 will produce small changes in the target variables, local-interpolation-like
 techniques are used to make predictions.
\end_layout

\begin_layout Section
1.5 Decision Theory 
\end_layout

\begin_layout Standard
to make optimal decisons in situations involving uncertainty such as those
 encountered in pattern recognition Problem description P39 
\end_layout

\begin_layout Standard
Minimizing the misclassification rate 
\end_layout

\begin_layout Standard
decision regions $R_k$ corresponding to $C_k$ decision boundaries/surfaces
 each x should be assigned to the class having the largest posterior probability
 $p(C_k|x)$ 
\end_layout

\begin_layout Standard
Minimizing the expected loss to minimize the total loss incurred loss/cost
 function, utility function, loss matrix 
\end_layout

\begin_layout Standard
The Reject option it may be appropriate to use an automatic system to classify
 those X-ray images for which there is little doubt as to the correct class,
 while leaving a human expert to classify the more ambiguous cases.
 introducing a threshold Inference and decision inference stage: in which
 we use training data to learn a model for $p(C_k|x)$ decision stage: make
 class assignments discriminant function: solve both problems together and
 simply learn a function that maps inputs x directly into decisions.
 
\end_layout

\begin_layout Standard
generative models(most demanding), discriminative models, discriminant function(
even simpler) (P43) ??? 
\end_layout

\begin_layout Standard
situations where the posterior probabilities are required to be computed
 P45 
\end_layout

\begin_layout Standard
loss functions for regression the conditional average of t conditioned on
 x , known as the regression function
\end_layout

\begin_layout Section
1.6 Information theory 
\end_layout

\begin_layout Standard
the amound of information can be viewed as the "degree of surprise" the
 concept of entropy: the average amount of information P50, two examples
 
\end_layout

\begin_layout Standard
noiseless coding theorem: the entropy is a lower bound on the number of
 bits needed to transmit the state of a random variable 
\end_layout

\begin_layout Chapter
2.
 Probability Distribution
\end_layout

\begin_layout Standard

\emph on
Density estimation
\end_layout

\begin_layout Standard
Data points are independent and identically distributed.
 There are infinitely many probability distributions that could have given
 rise to the observed finite data set.
\end_layout

\begin_layout Standard

\series bold
Parametric
\series default
 and 
\series bold
non-parametric
\series default
 approaches.
\end_layout

\begin_layout Section
2.1 Binary Variables 
\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $x\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $0\leq\mu\leq1$
\end_inset

, the one-coin-tossing distribution, i.e.
 the Bernoulli distribution
\begin_inset Formula 
\begin{equation}
\text{\text{Bern}\left(x|\mu\right) \ensuremath{=\mu^{x}\left(1-\mu\right){}^{1-x}}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
whose 
\begin_inset Formula $\mu_{ML}=\dfrac{1}{N}\sum\limits _{n=1}^{N}x_{n}=\dfrac{m}{N}$
\end_inset

, 
\begin_inset Formula $m$
\end_inset

 is the number of observations.
 
\end_layout

\begin_layout Standard
Suppose we toss this coin 
\begin_inset Formula $N$
\end_inset

 times, the number of heads is 
\begin_inset Formula $m$
\end_inset

, the distribution of 
\begin_inset Formula $m$
\end_inset

 will be
\begin_inset Formula 
\begin{equation}
\text{Bin}\left(m|N,\mu\right)=\dbinom{N}{m}\mu^{m}\left(1-\mu\right){}^{N-m}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
If we obtain 
\begin_inset Formula $m=N$
\end_inset

 in an experiment, the estimation will be unreasonable for the both distribution
s.
\end_layout

\begin_layout Subsection
2.1.1 The beta distribution 
\end_layout

\begin_layout Standard
To solve the problem of the unreasonble estimation (overfitting) when 
\begin_inset Formula $m=N$
\end_inset

, we introduce a prior to be proportional to powers of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $1-\mu$
\end_inset

, which results the same functional form of the posterior as the prior (
\shape italic
conjugacy
\shape default
), this prior we choose is 
\shape italic
beta
\shape default
 distribution:
\begin_inset Formula 
\begin{equation}
\text{Beta}\left(\mu|a,b\right)=\frac{\Gamma\left(a+b\right)}{\Gamma\left(a\right)\Gamma\left(b\right)}\mu^{a-1}\left(1-\mu\right){}^{b-1}
\end{equation}

\end_inset

where the gamma function 
\begin_inset Formula 
\begin{equation}
\Gamma\left(z\right)=\int_{0}^{\infty}x^{z-1}e^{-x}dx
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The parameters 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 control the distribution of the parameter 
\begin_inset Formula $\mu$
\end_inset

, thus called 
\shape italic
hyperparameters.
\end_layout

\begin_layout Standard
Multiplying (0.1) by (0.2) yields the posterior 
\begin_inset Formula 
\[
p\left(\mu|m,l,a,b\right)\propto\dbinom{l+m}{m}\mu^{m+a-1}\left(1-\mu\right){}^{l+b-1}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $l=N-m$
\end_inset

, taking the same form as the prior.
 The parameter estimation now depends both its own (prior) distribution
 and the data observed, and this gives rise to sequential learning, by updating
 
\begin_inset Formula $\mu$
\end_inset

 repeatedly with 
\begin_inset Formula $l$
\end_inset

 and 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Standard
In Bayesian learning, as we observe more and more data, the uncertainty
 represented by the posterior distribution will steadily decrese.
\end_layout

\begin_layout Standard
Given a parameter 
\begin_inset Formula $\theta$
\end_inset

 for which we have observed a data set 
\begin_inset Formula $D$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
E_{\theta}(\theta) & =E_{D}\underbrace{\left[E_{\theta}\left[\theta|D\right]\right]}_{\text{posterior mean}}\\
\text{var}_{\theta}[\theta] & =E_{D}\left[\text{var}_{\theta}\left[\theta|D\right]\right]+\text{var}_{D}\left[E_{\theta}\left[\theta|D\right]\right]
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
On average, the expectation of the prior expectation becomes more stable
 and the posterior variance of 
\begin_inset Formula $\theta$
\end_inset

 is smaller than the prior (less uncertainty).
\end_layout

\begin_layout Section
2.2 Multinomial Variables
\end_layout

\begin_layout Standard
The probability of 
\begin_inset Formula $x_{k}=1$
\end_inset

 is denoted by the parameter 
\begin_inset Formula $\mu_{k}$
\end_inset

, then the distribution
\begin_inset Formula 
\begin{equation}
p\left(x|\mu\right)=\prod_{k=1}^{K}\mu_{k}^{x_{k}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathrm{x}=\left(\dots,x_{k},\dots\right){}^{T}$
\end_inset

and 
\begin_inset Formula $\mu=\left(\mu_{1},\dots,\mu_{K}\right){}^{T}$
\end_inset

 and 
\begin_inset Formula $\sum\limits _{k}\mu_{_{k}}=1,\sum\limits _{k}x_{k}=1,\mathbb{E}\left[\mathrm{x}|\mu\right]=\sum\limits _{x}p\left(\mathrm{x}|\mu\right)\mathrm{x}=\mu$
\end_inset


\end_layout

\begin_layout Standard
Given a data set 
\begin_inset Formula $D$
\end_inset

 of 
\begin_inset Formula $N$
\end_inset

 observations, to use KKT conditions to maximize its MAL
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{k=1}^{K}m_{k}\ln\mu_{k}+\lambda\left(\sum_{k=1}^{K}\mu_{k}-1\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $m_{k}$
\end_inset

is the number of occurrence of 
\begin_inset Formula $x_{k}=1$
\end_inset

, yields 
\begin_inset Formula $\mu_{k}^{ML}=\dfrac{m_{k}}{N}$
\end_inset

.
\end_layout

\begin_layout Standard
The joint distribution of 
\begin_inset Formula $m_{1},\dots,m_{K}$
\end_inset

 conditioned on the parameters 
\begin_inset Formula $\mu$
\end_inset

 and the total number of observations 
\begin_inset Formula $N$
\end_inset


\begin_inset Formula 
\begin{align}
\text{Mult}\left(m_{1},\dots,m_{k}|\mu,N\right) & =\frac{N!}{m_{1}!m_{2}!\dots m_{k}!}\prod_{k=1}^{K}\mu_{k}^{m_{k}}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
subject to
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\prod_{k=1}^{K}m_{k}=N
\]

\end_inset


\end_layout

\begin_layout Subsection
2.2.1 The Dirichlet distribution
\end_layout

\begin_layout Standard
A family of prior distribution for the parameters 
\begin_inset Formula $\left\{ \mu_{k}\right\} $
\end_inset

 of the multinomial distribution, the 
\shape italic
dirichlet distribution
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\text{Dir}(\mu|\alpha)=\frac{\Gamma\left(\sum_{k}\alpha_{k}\right)}{\Gamma\left(\alpha_{1}\right)\dots\Gamma\left(\alpha_{K}\right)}\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Multiply (0.7) by (0.6)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
p\left(\mu|D,\alpha\right) & \propto\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}+m_{k}+1}\\
p\left(\mu|D,\alpha\right) & =\text{Dir}\left(\mu|\alpha+m\right)=\frac{\Gamma\left(N+\sum_{k}\alpha_{k}\right)}{\Gamma\left(\alpha_{1}+m_{1}\right)\dots\Gamma\left(\alpha_{K}+m_{k}\right)}\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}+m_{k}-1}
\end{align}

\end_inset


\end_layout

\begin_layout Section
2.3 The Gaussian Distribution
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
N\left(x|\mu,\sigma^{2}\right)=\dfrac{1}{\left(2\pi\right){}^{D/2}}\dfrac{1}{\left|\Sigma\right|{}^{1/2}}\exp\left\{ -\dfrac{1}{2}\left(x-\mu\right){}^{T}\Sigma^{-1}\left(x-\mu\right)\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\emph on
Mahalanobis distance
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $\left(x-\mu\right){}^{T}\Sigma^{-1}\left(x-\mu\right)$
\end_inset


\end_layout

\begin_layout Subsection
2.3.1 Conditional and Marginal Gaussian distributions
\end_layout

\begin_layout Standard
An important property of the multivariate Gaussian distribution is that
 if two sets of variables are jointly Gaussian, then the conditional distributio
n of one set conditoned on the other is again Gaussian.
 Similarly, the marginal distribution of either set is also Gaussian.
\end_layout

\begin_layout Subsection
2.3.3 Bayes' theorem for Gaussian variables
\end_layout

\begin_layout Standard
Given a marginal Gaussian distribution 
\begin_inset Formula $p\left(x\right)$
\end_inset

 for 
\begin_inset Formula $x$
\end_inset

 and a condition Gaussian distribution 
\begin_inset Formula $p\left(y|x\right)$
\end_inset

 for 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

, the marginal distribution of 
\begin_inset Formula $y$
\end_inset

 and the conditional distribution of 
\begin_inset Formula $x$
\end_inset

 given 
\begin_inset Formula $y$
\end_inset

 are also Gaussian.
\end_layout

\begin_layout Subsection
2.3.4 Maximum likelihood for the Gaussian
\end_layout

\begin_layout Standard
Given a data set 
\begin_inset Formula $X=\left(x_{1},...,x_{N}\right){}^{T}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\ln p\left(X|\mu,\Sigma\right)=-\dfrac{ND}{2}\ln\left(2\pi\right)-\dfrac{N}{2}\ln\left|\Sigma\right|-\dfrac{1}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{T}\Sigma^{-1}(x_{n}-\mu)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Take the derivative w.r.t.
 
\begin_inset Formula $\mu$
\end_inset

and set it to zero
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mu_{ML}=\dfrac{1}{N}\sum_{n=1}^{N}x_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
And the covariance (Magnus and Neudecker (1999))
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Sigma_{ML}=\dfrac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})(x_{n}-\mu_{ML})^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
which is biased.
\end_layout

\begin_layout Subsection
2.3.5 Sequential estimation
\end_layout

\begin_layout Standard
Robbins-Monro algorithm (Robbins and Monro,1951; Fukunaga,1990)
\end_layout

\begin_layout Subsection
2.3.6 Bayesian inference for the Gaussian
\end_layout

\begin_layout Paragraph
Given variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

 and a set of 
\begin_inset Formula $N$
\end_inset

 observations 
\begin_inset Formula $X=\left\{ x_{1},\dots,x_{N}\right\} $
\end_inset

, the mean 
\begin_inset Formula $\mu$
\end_inset

 to be inferred
\end_layout

\begin_layout Standard
Take the prior distribution to be 
\begin_inset Formula 
\begin{equation}
p\left(\mu\right)=\mathcal{N}\left(\mu|\mu_{0},\sigma_{0}^{2}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The posterior distribution becomes
\begin_inset Formula 
\begin{equation}
p\left(\mu|\mathrm{X}\right)=\mathcal{N}\left(\mu|\mu_{N},\sigma_{N}^{2}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula 
\begin{align}
\mu_{N} & =\frac{\sigma^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\mu_{0}+\frac{N\sigma_{0}^{2}}{N\sigma_{0}^{2}+\sigma^{2}}\mu_{ML}\\
\frac{1}{\sigma_{N}^{2}} & =\frac{1}{\sigma_{0}^{2}}+\frac{N}{\sigma^{2}}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Note how the paramter change as 
\begin_inset Formula $N\rightarrow0/\infty,\sigma_{0}^{2}\rightarrow\infty$
\end_inset

: the precision increases and the posterior distribution becomes infintely
 peaked around the MAL solution.
\end_layout

\begin_layout Subparagraph
Sequential learning 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p\left(\mu|D_{N}\right)\propto\underbrace{\left[p\left(\mu\right)\prod_{n=1}^{N-1}p\left(\mathrm{x_{n}|\mu}\right)\right]}_{p\left(\mu|D_{N-1}\right)}p\left(\mathrm{x_{N}|\mu}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Paragraph
Given variance 
\begin_inset Formula $\mu$
\end_inset

 and a set of 
\begin_inset Formula $N$
\end_inset

 observations 
\begin_inset Formula $X=\left\{ x_{1},\dots,x_{N}\right\} $
\end_inset

, the mean 
\begin_inset Formula $\sigma^{2}$
\end_inset

 or 
\begin_inset Formula $\lambda\equiv\dfrac{1}{\sigma^{2}}$
\end_inset

 to be inferred
\end_layout

\begin_layout Standard

\emph on
\begin_inset CommandInset href
LatexCommand href
name "Gamma distribution"
target "https://en.wikipedia.org/wiki/Gamma_distribution"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
The likelihood
\begin_inset Formula 
\begin{equation}
p\left(\mathrm{X|\lambda}\right)\propto\lambda^{N/2}\exp\left\{ -\frac{\lambda}{2}\sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2}\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The prior is a 
\emph on
gamma
\emph default
 distribution
\begin_inset Formula 
\begin{equation}
\text{Gam}\left(\lambda|a,b\right)=\frac{1}{\Gamma\left(a\right)}b^{a}\lambda^{a-1}\exp\left(-b\lambda\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The posterior 
\begin_inset Formula 
\begin{equation}
p\left(\lambda|\mathrm{X}\right)\propto\text{Gam}\left(\lambda|a_{N},b_{N}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula 
\begin{align}
a_{N} & =a_{0}+\frac{N}{2}\\
b_{N} & =b_{0}+\frac{N}{2}\sigma_{ML}^{2}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Another prior distribution 
\emph on
can be inverse gamma
\end_layout

\begin_layout Paragraph
Consider the case where both parameters are unknown
\end_layout

\begin_layout Standard
The likelihood can be written as
\begin_inset Formula 
\begin{equation}
p\left(\mu,\lambda\right)=\mathcal{N}\left(\mu|\mu_{0},\left(\beta\lambda\right)^{-1}\right)\text{Gam}\left(\lambda|a,b\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mu_{0}=c/\beta,a=1+\beta/2,b=d-c^{2}/2\beta$
\end_inset

.
 This distribution is called the 
\emph on
normal-gamma
\emph default
 or 
\emph on
Gaussian-gamma
\emph default
 distribution.
\end_layout

\begin_layout Paragraph
In the case the multivariate Gaussian distribution
\end_layout

\begin_layout Standard
For known mean and unknown precision matrix, the conjugata prior is the
 
\emph on
Wishart 
\emph default
distribution.
 If both are unknown, this goes to the 
\emph on
normal-Wishart
\emph default
 or 
\emph on
Gaussian-Wishart
\emph default
 distribution.
\end_layout

\begin_layout Subsection
2.3.9 Mixtures of Gaussians
\end_layout

\begin_layout Standard

\emph on
Mixture distributions
\emph default
: probabilistic models formed by taking linear combinations of more basic
 distributions.
\end_layout

\begin_layout Standard

\emph on
mixture of Gaussians
\emph default
: 
\begin_inset Formula 
\begin{equation}
p\left(x\right)=\sum_{k=1}^{K}\pi_{k}N\left(x|\mu_{k},\Sigma_{k}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where each Gaussian density is called a 
\emph on
component 
\emph default
which has its own mean and covariance, and 
\begin_inset Formula $\pi_{k}$
\end_inset

's are called 
\emph on
mixing coefficients 
\emph default
with 
\begin_inset Formula 
\begin{align}
\sum_{k=1}^{K}\pi_{k} & =1\\
0\leq\pi_{k} & \leq1
\end{align}

\end_inset


\end_layout

\begin_layout Standard
From the sum and product rules
\begin_inset Formula 
\begin{equation}
p\left(x\right)=\sum_{k=1}^{K}p\left(k\right)p\left(x|k\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
equivalent the definition.
 The posterior distribution 
\begin_inset Formula $p\left(x|k\right)$
\end_inset

 are called 
\emph on
responsibilities
\emph default
 and from the Bayes' theorem
\begin_inset Formula 
\[
p\left(x|k\right)=\frac{\pi_{k}N\left(x|\mu_{k},\Sigma_{k}\right)}{\sum_{l}\pi_{l}N\left(x|\mu_{l},\Sigma_{l}\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
The MAL for mixture Gaussians has no closed-form analytical solution.
 iterative numerical optimzation techniques may be used or 
\emph on
expectation maximization.
\end_layout

\begin_layout Section
2.4 The Expoential Family
\end_layout

\begin_layout Standard
The exponential family of distributions over 
\begin_inset Formula $x$
\end_inset

, given parameters 
\begin_inset Formula $\eta$
\end_inset

, is defined to be the set of distributions of the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(x|\eta)=h\left(x\right)g\left(\eta\right)\exp\left\{ \eta^{T}u(x)\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\eta$
\end_inset

 are called the 
\emph on
natural parameters.
 
\emph default
Since the distribution is normalized 
\begin_inset Formula 
\begin{equation}
g\left(\eta\right)\int h\left(x\right)\exp\left\{ \eta^{T}u(x)\right\} dx=1
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
For the Bernoulli distribution
\begin_inset Formula 
\begin{equation}
p(x|\eta)=\text{sigmoid}\left(-\eta\right)\exp\left(\eta x\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\eta=\ln\dfrac{\mu}{1-\mu}.$
\end_inset


\end_layout

\begin_layout Standard
For the multinomial distribution
\begin_inset Formula 
\begin{equation}
p\left(x|\eta\right)=\exp\left(\eta^{T}x\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\eta_{k}=\ln\mu_{k}$
\end_inset


\end_layout

\begin_layout Standard
Or 
\begin_inset Formula 
\begin{equation}
p(x|\eta)=\left(1+\sum_{k=1}^{M-1}\exp\left(\eta_{k}\right)\right)^{-1}\exp\left(\eta^{T}x\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mu_{k}=\dfrac{\exp\left(\eta_{k}\right)}{1+\sum_{j}\exp\left(\eta_{j}\right)}$
\end_inset

, called 
\emph on
softmax 
\emph default
or 
\emph on
normalized exponential.
\end_layout

\begin_layout Subsection
2.4.1 Maximum likelihood and sufficient statistics
\end_layout

\begin_layout Standard

\emph on
\begin_inset CommandInset href
LatexCommand href
name "Sufficient statistics"
target "https://en.wikipedia.org/wiki/Sufficient_statistic"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Take the gradient of both sides of the integrated exponential distribution
\begin_inset Formula 
\[
-\triangledown\ln g\left(\eta\right)=\mathbb{E}\left[u\left(x\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
By MAL
\begin_inset Formula 
\[
-\triangledown\ln g\left(\eta\right)=\frac{1}{N}\sum_{n=1}^{N}u\left(x_{n}\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
2.4.1 Conjugate priors
\end_layout

\begin_layout Standard
For any member of the exponential family, there exists a conjugate prior
 
\begin_inset Formula 
\[
p\left(\eta|\chi,\nu\right)=f\left(\chi,\nu\right)g\left(\eta\right)^{\nu}\exp\left\{ \nu\eta^{T}\chi\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $f\left(\chi,\nu\right)$
\end_inset

 is a normalization coefficients.
\end_layout

\begin_layout Standard
The posterior distribution becomes
\begin_inset Formula 
\[
p\left(\eta|X,\chi,\nu\right)\propto g\left(\eta\right)^{\nu+N}\exp\left\{ \eta^{T}\left(\sum_{n=1}^{N}u\left(x_{n}\right)+\nu\chi\right)\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
taking the same form, confirming conjugacy.
\end_layout

\begin_layout Section
2.5 Nonparametric Methods
\end_layout

\begin_layout Standard
An important limitation of the 
\emph on
parametric 
\emph default
approach to density estimation is that the chosen density might be a poor
 model of the distribution that generates the data, which can result in
 poor predictive performance.
 The nonparametric Bayesian methods are attracting increasing interest.
\end_layout

\begin_layout Paragraph
The histogram density models
\begin_inset Formula 
\[
p_{i}=\frac{n_{i}}{N\Delta_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\Delta_{i}$
\end_inset

 is the width of the 
\begin_inset Formula $i$
\end_inset

th bin.
\end_layout

\begin_layout Standard
The widths of bins, and the choice of edge location for the bins affect
 a histogram density model.
 Also it suffers from the 
\emph on
curse of dimension
\emph default
, a total number of 
\begin_inset Formula $M^{D}$
\end_inset

 bins in a 
\begin_inset Formula $D$
\end_inset

-dimensional space with each variable divided into 
\begin_inset Formula $M$
\end_inset

 bins.
\end_layout

\begin_layout Subsection
2.5.2 Nearest-neighbor methods
\end_layout

\begin_layout Standard
We consider a fixed value of 
\begin_inset Formula $K$
\end_inset

 in a neighborhood, and use the data to find an appropriate value for 
\begin_inset Formula $V$
\end_inset

 (volume of the neighborhood).
 Center a small sphere at the point 
\begin_inset Formula $x$
\end_inset

 at which the density 
\begin_inset Formula $p\left(x\right)$
\end_inset

 is estimated.
 Let the radius of the sphere to grow until contains precisely 
\begin_inset Formula $K$
\end_inset

 data points.
 Then
\begin_inset Formula 
\[
p\left(x\right)=\frac{K}{NV}
\]

\end_inset

 The value of 
\begin_inset Formula $K$
\end_inset

 governs the degree of smoothing and there is an optimum choice for 
\begin_inset Formula $K$
\end_inset

 that is neither too large nor too small.
\end_layout

\begin_layout Paragraph
KNN for classification
\end_layout

\begin_layout Standard
We apply the estimate the density of each class in the neighborhood obtained
 and make use of Bayes' theorem.
\end_layout

\begin_layout Standard
A data set comprising 
\begin_inset Formula $N_{k}$
\end_inset

 points in class 
\begin_inset Formula $C_{k}$
\end_inset

 with 
\begin_inset Formula $N$
\end_inset

 points in total.
 For a new point 
\begin_inset Formula $x$
\end_inset

, draw a sphere contered at it containing exactly 
\begin_inset Formula $K$
\end_inset

 points irrespective of their class, then
\begin_inset Formula 
\begin{align*}
p\left(x|C_{k}\right) & =\frac{K_{k}}{N_{k}V}\\
p\left(x\right) & =\frac{K}{NV}\\
p\left(C_{k}\right) & =\frac{N_{k}}{N}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Through the Bayes' theorem
\begin_inset Formula 
\[
p\left(C_{k}|x\right)=\frac{p\left(x|C_{k}\right)p\left(C_{k}\right)}{p\left(x\right)}=\frac{K_{k}}{K}
\]

\end_inset


\end_layout

\begin_layout Standard
To minimize the probability of misclassification, 
\begin_inset Formula $x$
\end_inset

 is assigned to the class having the largest posterior porbability.
\end_layout

\begin_layout Standard
The particular case of 
\begin_inset Formula $K=1$
\end_inset

 is called the 
\emph on
nearest-neighbor
\emph default
 rule.
\end_layout

\begin_layout Chapter
3.
 Linear Models for Regression
\end_layout

\begin_layout Standard
The goal of regression is to predict the value of one or more continuous
 
\emph on
target
\emph default
 variables 
\begin_inset Formula $t$
\end_inset

 given the value of a 
\begin_inset Formula $D$
\end_inset

-dimensional vector 
\begin_inset Formula $\mathrm{x}$
\end_inset

 
\emph on
input
\emph default
 variables.
\end_layout

\begin_layout Section
3.1 Linear Basis Function Models
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
y(x,w)= & w_{0}+\sum\limits _{j=1}^{M-1}w_{j}\phi_{j}(x)\\
= & w^{T}\phi(x)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\phi_{j}(\mathrm{x})$
\end_inset

 are known as 
\emph on
basis functions
\emph default
, and 
\begin_inset Formula $w_{0}$
\end_inset

 
\emph on
bias
\emph default
 parameter
\end_layout

\begin_layout Standard
\begin_inset Formula $w=(w_{0},...,w_{M-1})^{T}$
\end_inset

 and 
\begin_inset Formula $\phi=(\phi_{0},...,\phi_{M-1})^{T}$
\end_inset

.
\end_layout

\begin_layout Standard
By using nonlinear basis functions, we allow the function 
\begin_inset Formula $y(x,w)$
\end_inset

 to be a non-linear function of the input vector 
\begin_inset Formula $x$
\end_inset

.
 Thus the equation above is called a 
\emph on
linear model
\emph default
.
\end_layout

\begin_layout Standard

\series bold
Choices for the basis functions
\end_layout

\begin_layout Standard

\emph on
Gaussian
\emph default
:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j}(x)=\exp\Bigg\{-\dfrac{(x-\mu_{j})^{2}}{2s^{2}}\Bigg\}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mu_{j}$
\end_inset

 govern the locations of the basis functions in input space.
\end_layout

\begin_layout Standard

\emph on
Sigmoidal basis function
\emph default
:
\begin_inset Formula 
\[
\phi_{j}(x)=\sigma(\dfrac{x-\mu_{j}}{s}):\sigma(a)=\dfrac{1}{1+\exp(-a)}
\]

\end_inset


\end_layout

\begin_layout Standard
and 
\begin_inset Formula $tanh(a)=2\sigma(a)-1$
\end_inset


\end_layout

\begin_layout Standard

\emph on
the Fourier basis
\emph default
: Each basis function represents a specific frequency and has infinite spatial
 extent.
\end_layout

\begin_layout Paragraph
Maximum Likelihood and least squares
\end_layout

\begin_layout Standard

\emph on
\begin_inset CommandInset href
LatexCommand href
name "Mode"
target "https://en.wikipedia.org/wiki/Mode_(statistics)"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Assume 
\begin_inset Formula $p(t,|X,w,\beta)=\mathcal{N}(t|y,\beta^{-1})$
\end_inset

, where 
\begin_inset Formula $t=y\left(w,x\right)+\epsilon$
\end_inset

 and 
\begin_inset Formula $y=w^{T}\phi(x)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\ln p(t|w,\beta)= & \sum_{n=1}^{N}\ln N\left(t_{n}|w^{T}\phi\left(x_{n}\right),\beta^{-1}\right)\\
= & \dfrac{N}{2}\ln\beta-\dfrac{N}{2}\ln(2\pi)-\beta E_{D}(w)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathrm{t}$
\end_inset

 is the column vector of targets and 
\begin_inset Formula $E_{D}(w)=\dfrac{1}{2}\sum\limits _{n=1}^{N}\left\{ t_{n}-w^{T}\phi(x_{n})\right\} {}^{2}$
\end_inset

.
 It is easy to see that maximization of the likelihood function under a
 conditional Gaussian noise distribution for a linear model is equivalent
 to minimizing a sum-of-squares error function.
 Take the gradient and set it to zero:
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Formula $y=Ax\rightarrow\triangledown y=x$
\end_inset

, 
\begin_inset Formula $W=x^{T}Ax\rightarrow\triangledown W=2x^{T}A$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w_{ML}=(\Phi^{T}\Phi)^{-1}\Phi^{T}\mathrm{t}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\emph on
design matrix
\emph default
 
\begin_inset Formula $\Phi$
\end_inset

 is given by 
\begin_inset Formula $\Phi_{nj}=\phi_{j}(x_{n})$
\end_inset


\end_layout

\begin_layout Standard
Take the derivative w.r.t.
 
\begin_inset Formula $w_{0}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w_{0}=\bar{t}-\sum\limits _{j=1}^{N}w_{j}\bar{\phi}_{j}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\bar{t}$
\end_inset

 and 
\begin_inset Formula $\bar{\phi_{j}}$
\end_inset

 are the arithmetic mean of their elements.
\end_layout

\begin_layout Standard
Maximize the log likehood w.r.t.
 the noise precision parameter 
\begin_inset Formula $\beta$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\dfrac{1}{\beta_{ML}}=\dfrac{1}{N}\sum\limits _{n=1}^{N}[t_{n}-w_{ML}^{T}\phi(x_{n})]^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
The least-squares regression function (Euclidean distance) is obtained by
 finding the orthogonal projection of the data vector 
\begin_inset Formula $t$
\end_inset

 onto the subspace spanned by the basis functions 
\begin_inset Formula $\phi_{j}(x)$
\end_inset

 in which each basis function is viewed as a vector 
\begin_inset Formula $\phi_{j}$
\end_inset

 of length 
\emph on
N
\emph default
 with elements 
\begin_inset Formula $\phi_{j}(x_{n})$
\end_inset

.
\end_layout

\begin_layout Paragraph
Sequential learning (online algorithms)
\end_layout

\begin_layout Standard

\series bold
Stochastic graidient descent (sequential gradient descent)
\end_layout

\begin_layout Standard

\emph on
\begin_inset CommandInset href
LatexCommand href
name "Gradient descent"
target "https://en.wikipedia.org/wiki/Gradient_descent"
literal "false"

\end_inset


\emph default
, see the description.
\end_layout

\begin_layout Standard
Given an error function 
\begin_inset Formula $E=\sum_{n}E_{n}$
\end_inset

 , a sum over data points, after presentation of pattern 
\begin_inset Formula $n$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w^{(t+1)}=w^{(t)}-\eta\triangledown E_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $t$
\end_inset

 is the iteration number and 
\begin_inset Formula $\eta$
\end_inset

 is a 
\emph on
learning rate
\emph default
.
\end_layout

\begin_layout Standard

\emph on
LMS (least-mean-squares) algorithm
\end_layout

\begin_layout Standard
For the case of the sum-of-squares error function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\triangledown E_{n}=(t_{n}-w^{(t)T}\phi_{n})\phi_{n}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Regularized least squares
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(x)=E_{D}+\lambda E_{W}
\]

\end_inset


\end_layout

\begin_layout Standard
In general 
\begin_inset Formula $E_{W}=\dfrac{\lambda}{2}\sum\limits _{j=1}^{M}|w_{j}|^{q}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $q=1$
\end_inset

: (lasso) if is large enough, some of the coefficients are driven to zero
\begin_inset Newline newline
\end_inset

 
\begin_inset Formula $q=2$
\end_inset

: 
\begin_inset Formula $E_{w}(q=2)$
\end_inset

 is known in ML as 
\emph on
weight decay
\emph default
, because in sequential learning algorithms, it encourages weight values
 to decay towards zero.
 In statistics, it is an example of a 
\emph on
parameter shrinkage
\emph default
 method.
 
\begin_inset Formula $w=(\lambda I+\Phi^{T}\Phi)^{-1}\Phi^{T}t$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/djn_dl/Desktop/GitHub/Commentarii/PRML/1524666731026.png
	lyxscale 50

\end_inset

 
\end_layout

\begin_layout Standard
Think about this figure in a Langrange-multipliers way.
\end_layout

\begin_layout Paragraph
Multiple outputs
\end_layout

\begin_layout Standard
Of course we can decouple into multiple independent regression problems,
 however there is an approach using the same set of basis functions so that
 y
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y(x,w)=W^{T}\phi(x)
\]

\end_inset


\end_layout

\begin_layout Standard
and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(\mathrm{t}|x,W,\beta)=\mathcal{N}(\mathrm{t}|W^{T}\phi(x),\beta^{-1}I)
\]

\end_inset


\end_layout

\begin_layout Standard
which yields
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
W_{ML}=\Phi^{\dagger}T
\]

\end_inset


\end_layout

\begin_layout Standard
For the case of arbitrary covariance matrices, see MAL of multi-variate
 Gaussian distribution.
\end_layout

\begin_layout Section
3.2 The Bias-Variance Decomposition
\end_layout

\begin_layout Chapter
4.
 Linear Models for Classification
\end_layout

\begin_layout Standard
The goal in classification is to take an input vector 
\begin_inset Formula $\mathrm{x}$
\end_inset

 and to assign it to one of 
\begin_inset Formula $K$
\end_inset

 discrete classes 
\begin_inset Formula $\mathcal{C}_{k}$
\end_inset

 where 
\begin_inset Formula $k=1,\dots,K$
\end_inset

.
 The input space is divided into 
\emph on
decision regions
\emph default
 whose boundaries are called 
\emph on
decision boundaries
\emph default
 or 
\emph on
decision surfaces
\emph default
.
 Data sets whose classes can be separated exactly by linear decision surfaces
 are said to be 
\emph on
linearly separable.
 
\emph default
For probabilistic models, the most convenient is 1-of-
\begin_inset Formula $K$
\end_inset

 coding scheme.
\end_layout

\begin_layout Standard
There are three distinct approaches to the classification problem: the simplest
 constructs a 
\emph on
discriminant function 
\emph default
that directly assigns each vector 
\begin_inset Formula $\mathrm{x}$
\end_inset

 to a specfic class.
 A more powerful approach models the conditional probability 
\begin_inset Formula $p\left(\mathcal{C}_{k}|\mathrm{x}\right)$
\end_inset

 in an inference stage and then uses this distribution to make optimal decisions.
 One way to the model is to model directly, the other is to adopt a generative
 approach, i.e., to model the class-conditional densities 
\begin_inset Formula $p\left(x|C_{k}\right)$
\end_inset

 and the prior probabilities 
\begin_inset Formula $p\left(C_{k}\right)$
\end_inset

 for the classes and use Bayes' theorem
\begin_inset Formula 
\[
p\left(C_{k}|x\right)=\frac{p\left(x|C_{k}\right)p\left(C_{k}\right)}{p\left(x\right)}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Generalized Linear Models
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y\left(x\right)=f\left(w^{T}x+w_{0}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is an nonlinear 
\emph on
activation function
\emph default
 whereas its inverse is called a 
\emph on
link function
\emph default
.
\end_layout

\begin_layout Section
4.1 Discriminant Functions
\end_layout

\begin_layout Subsection
4.1.1 Two classes
\end_layout

\begin_layout Standard
A linear dicriminant function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y\left(x\right)=w^{T}x+w_{0}=\tilde{w}^{T}\tilde{x}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\tilde{x}=\left(x_{0},\mathrm{x}\right)$
\end_inset

 and 
\begin_inset Formula $\tilde{w}=\left(w_{0},\mathrm{w}\right)$
\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $y\left(x\right)\geq0,$
\end_inset

then 
\begin_inset Formula $x\in C_{1}$
\end_inset

 otherwise 
\begin_inset Formula $x\in C_{2}$
\end_inset

.
 The decision boundary is defined by 
\begin_inset Formula $y\left(x\right)=0$
\end_inset

, a 
\begin_inset Formula $\left(D-1\right)$
\end_inset

-dimensional hyperplane within the 
\begin_inset Formula $D$
\end_inset

-dimensional input space.
 The 
\emph on
weight vector 
\emph default

\begin_inset Formula $w$
\end_inset

 is orthogonal to the hyperplane.
 
\end_layout

\begin_layout Standard
Set 
\begin_inset Formula $x=0$
\end_inset

, we have
\begin_inset Formula 
\[
\frac{w^{T}x}{\left\Vert w\right\Vert }=-\frac{w_{0}}{\left\Vert w\right\Vert }
\]

\end_inset

 Thus 
\begin_inset Formula $w_{0}$
\end_inset

 determines the location of the decision surface.
\end_layout

\begin_layout Subsection
4.1.2 Multiple classes
\end_layout

\begin_layout Standard
\begin_inset Formula $K$
\end_inset

-classes
\end_layout

\begin_layout Standard

\emph on
One-versus-the-rest:
\emph default
 the use of 
\begin_inset Formula $K-1$
\end_inset

 classifers each of which solves a two-class problem of separating points
 in a particular class 
\begin_inset Formula $C_{k}$
\end_inset

 from points not in that class.
 
\end_layout

\begin_layout Standard

\emph on
One-versus-one: 
\emph default
introduces 
\begin_inset Formula $K\left(K-1\right)/2$
\end_inset

 binary discriminant functions, one for every possible pair of classes.
\end_layout

\begin_layout Standard
The above two approaches run into the problem of ambiguous regions.
\end_layout

\begin_layout Standard
To avoid the difficulty, consider a single 
\begin_inset Formula $K$
\end_inset

-class discriminant comprising 
\begin_inset Formula $K$
\end_inset

 linear functions of the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{k}\left(\text{x}\right)=\text{w}_{k}^{T}\text{x}+w_{k0}
\]

\end_inset


\end_layout

\begin_layout Standard
and then assign a point 
\begin_inset Formula $x$
\end_inset

 to class 
\begin_inset Formula $C_{k}$
\end_inset

 if 
\begin_inset Formula $y_{k}\left(x\right)>y_{j}\left(x\right)$
\end_inset

 for all 
\begin_inset Formula $j\neq k.$
\end_inset

 The decision boundary between 
\begin_inset Formula $C_{k}$
\end_inset

 and 
\begin_inset Formula $C_{j}$
\end_inset

 is given by 
\begin_inset Formula $y_{k}\left(x\right)=y_{j}\left(x\right)$
\end_inset

.
 The decison regions of such a discriminant can be proved singly connected
 and convex.
\end_layout

\begin_layout Subsection
4.1.3 Least squares for classification
\end_layout

\begin_layout Standard
One justification for using least squares in a general classfication problem
 with 
\begin_inset Formula $K$
\end_inset

 classes, with a 1-of-
\begin_inset Formula $K$
\end_inset

 binary coding scheme for the target vector 
\begin_inset Formula $t$
\end_inset

, is that it approximates the conditional expectation 
\begin_inset Formula $E\left[t|x\right]$
\end_inset

 of the target values given the input vector.
 For binary coding scheme, this conditional expectation expectation is given
 by the vector of posterior class probabilities which, unfortuately, give
 poor approximation.
\end_layout

\begin_layout Standard
Each class 
\begin_inset Formula $C_{k}$
\end_inset

 is described by its own linear model so that 
\begin_inset Formula 
\[
y_{k}\left(x\right)=w_{k}^{T}x+w_{k0}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $k=1,\dots,K$
\end_inset


\begin_inset Formula 
\[
\mathrm{y}\left(\mathrm{x}\right)=\tilde{\mathrm{W}}^{T}\tilde{\mathrm{x}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\tilde{\mathrm{W}}_{\cdot,k}=\left(w_{k0},\mathrm{w}_{k}^{T}\right)^{T}$
\end_inset

and 
\begin_inset Formula $\tilde{\mathrm{x}}=\left(1,\mathrm{\mathrm{x}^{T}}\right)^{T}$
\end_inset


\end_layout

\begin_layout Standard
Consider a training data set 
\begin_inset Formula $\left\{ x_{n},t_{n}\right\} $
\end_inset

 where 
\begin_inset Formula $n=1,\dots,N$
\end_inset

, target matrix 
\begin_inset Formula $T_{n,\cdot}=t_{n}^{T}$
\end_inset

, matrix 
\begin_inset Formula $\tilde{\mathrm{X}}_{n,\cdot}=\tilde{\text{x}}_{n}^{T}$
\end_inset

.
 The sum-of-squares error function can be written as 
\begin_inset Formula 
\[
E_{D}\left(\tilde{\mathrm{W}}\right)=\frac{1}{2}\text{Tr}\left\{ \left(\tilde{\mathrm{X}}\tilde{\mathrm{W}}-T\right)^{T}\left(\tilde{\mathrm{X}}\tilde{\mathrm{W}}-T\right)\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
Minimizing it, we obtain the solution for the coefficients
\begin_inset Formula 
\[
\tilde{\mathrm{W}}=\left(\tilde{\mathrm{X}}^{T}\tilde{\mathrm{X}}\right)^{-1}\tilde{\mathrm{X}}^{T}T
\]

\end_inset


\end_layout

\begin_layout Standard
If we use a 
\begin_inset Formula $1$
\end_inset

-of-
\begin_inset Formula $K$
\end_inset

 coding scheme for 
\begin_inset Formula $K$
\end_inset

 classes, the elements of 
\begin_inset Formula $y\left(x\right)$
\end_inset

 will sum to 1 for any value of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
The least-squares solutions lack robustness to outliers.
 The failure of least squares results from the assumption of the target
 being Gaussian, while in a classification problem this is not the case.
\end_layout

\begin_layout Subsection
4.1.4 Fisher's linear discriminant
\end_layout

\begin_layout Standard
One way to view a linear classification model is in terms of dimensionality
 reduction.
 Consider first the case of two classes, and suppose we project 
\begin_inset Formula $D$
\end_inset

-dimensional input vector 
\begin_inset Formula $x$
\end_inset

 to one dimension using
\begin_inset Formula 
\[
y=\text{w}^{T}\text{x}
\]

\end_inset


\end_layout

\begin_layout Standard
The idea proposed by Fisher is to maximize a function that will give a large
 separation between the projected class means while also giving a small
 variance within each class, thereby minimizing the class overlap.
 Define the mean and variance of each class
\begin_inset Formula 
\begin{align*}
m_{1} & =\frac{1}{N_{1}}\sum_{n\in C_{1}}x_{n}\quad m_{2}=\frac{1}{N_{2}}\sum_{n\in C_{2}}x_{n}\\
s_{k}^{2} & =\sum_{n\in C_{k}}\left(y_{n}-w^{T}m_{k}\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $y_{n}=w^{T}x_{n}$
\end_inset

.
 The Fisher criterion is defined by
\begin_inset Formula 
\[
J\left(w\right)=\frac{w^{T}(m_{2}-m_{1})}{s_{1}^{2}+s_{2}^{2}}=\frac{w^{T}S_{B}w}{w^{T}S_{W}w}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $S_{B}=\left(m_{2}-m_{1}\right)(m_{2}-m_{1})^{T},$
\end_inset

 and 
\begin_inset Formula $S_{W}=S_{1}+S_{2}$
\end_inset

, 
\begin_inset Formula $S_{1}$
\end_inset

 and 
\begin_inset Formula $S_{2}$
\end_inset

 are the within-class covariance matrix before transformation.
 Differentiate 
\begin_inset Formula $J\left(w\right)$
\end_inset

 w.r.t.
 
\begin_inset Formula $w$
\end_inset

,
\begin_inset Formula 
\[
\left(w^{T}S_{B}w\right)S_{W}w=\left(w^{T}S_{W}w\right)S_{B}w
\]

\end_inset


\end_layout

\begin_layout Standard
Ignore the magnitude of 
\begin_inset Formula $w$
\end_inset

, then 
\begin_inset Formula 
\[
w\propto S_{W}^{-1}(m_{2}-m_{1})
\]

\end_inset


\end_layout

\begin_layout Standard
Known as 
\emph on
Fishers's linear discriminant,
\emph default
 not strictly a discriminant though.
 The projected data can subsequently be used to construct a discriminant
 by choosing a threshold 
\begin_inset Formula $y_{0}$
\end_inset

.
\end_layout

\begin_layout Section
4.2 Probabilistic Generative Models
\end_layout

\begin_layout Standard
We model the class-conditional densities 
\begin_inset Formula $p\left(x|C_{k}\right)$
\end_inset

 as well as the class prior 
\begin_inset Formula $p\left(C_{k}\right)$
\end_inset

, and then use them to compute the posterior probabilities 
\begin_inset Formula $p\left(C_{k}|x\right)$
\end_inset

 through Bayes' theorem.
\end_layout

\begin_layout Paragraph
The case of two classes
\end_layout

\begin_layout Standard

\emph on
\begin_inset CommandInset href
LatexCommand href
name "Logistic function"
target "https://en.wikipedia.org/wiki/Logistic_function#Mathematical_properties"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
p\left(C_{1}|x\right) & =\frac{p\left(x|C_{1}\right)p\left(C_{1}\right)}{p\left(x|C_{1}\right)p\left(C_{1}\right)+p\left(x|C_{2}\right)p\left(C_{2}\right)}\\
 & =\frac{1}{1+\exp\left(-a\right)}=\text{sigmoid}\left(a\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula 
\[
a=\ln\frac{p\left(x|C_{1}\right)p\left(C_{1}\right)}{p\left(x|C_{2}\right)p\left(C_{2}\right)}
\]

\end_inset


\end_layout

\begin_layout Paragraph
The case of 
\begin_inset Formula $K>2$
\end_inset

 classes
\end_layout

\begin_layout Standard

\emph on
\begin_inset CommandInset href
LatexCommand href
name "Softmax function"
target "https://en.wikipedia.org/wiki/Softmax_function"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
p\left(C_{k}|x\right) & =\frac{p\left(x|C_{1}\right)p\left(C_{1}\right)}{\sum_{j}p\left(x|C_{j}\right)p\left(C_{j}\right)}\\
 & =\underbrace{\frac{\exp\left(a_{k}\right)}{\sum_{j}\exp\left(a_{j}\right)}}_{\text{softmax}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula 
\[
a_{k}=\ln p\left(x|C_{k}\right)p\left(C_{k}\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
4.2.1 Continuous inputs
\end_layout

\begin_layout Standard
Assume the class-conditional probabilities are all Gaussian
\begin_inset Formula 
\[
p\left(x|C_{k}\right)=N\left(x|\mu_{k},\Sigma\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Consider the case of two-classes, the posterior becomes
\begin_inset Formula 
\[
p\left(C_{1}|x\right)=\sigma\left(w^{T}x+w_{0}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula 
\begin{align*}
w & =\Sigma^{-1}\left(\mu_{1}-\mu_{2}\right)\\
w_{0} & =-\frac{1}{2}\mu_{1}\Sigma^{-1}\mu_{1}+\frac{1}{2}\mu_{2}\Sigma^{-1}\mu_{2}+\ln\frac{p\left(C_{1}\right)}{p\left(C_{2}\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And in the case of 
\begin_inset Formula $K$
\end_inset

 classes
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
a_{k}(x)=w_{k}^{T}x+w_{k0}
\]

\end_inset


\end_layout

\begin_layout Standard
where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
w_{k} & =\Sigma^{-1}\mu_{k}\\
w_{k0} & =-\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+\ln p\left(C_{k}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note that the 
\begin_inset Formula $a_{k}$
\end_inset

 here is not directly obtain from the second equation rather than the posterior
 equation, thus the quadratic forms are cancelled.
\end_layout

\begin_layout Standard
If weach class-conditional density 
\begin_inset Formula $p\left(x|C_{k}\right)$
\end_inset

 has its own covariance matrix, the cancellations no longer occurs, giving
 rise to a 
\emph on
quadratic discriminant.
\end_layout

\begin_layout Subsection
4.2.2 Maximum likelihood solution
\end_layout

\begin_layout Standard
Here we maximize the likehood of the posterior.
 Consider the case of two classes where the targets are scalars, either
 1 or 0.
 Denoting the prior 
\begin_inset Formula $p\left(C_{1}\right)=\pi$
\end_inset

 and 
\begin_inset Formula $p\left(C_{2}\right)=1-\pi$
\end_inset

, the likelihood function is given by 
\begin_inset Formula 
\[
p\left(\mathrm{t}|\pi,\mu_{1},\mu_{2},\Sigma\right)=\prod_{n=1}^{N}\left[\pi N\left(\mathrm{\mathrm{x}_{n}|\mu_{1},\Sigma}\right)\right]^{t_{n}}\left[\left(1-\pi\right)N\left(\mathrm{\mathrm{x}_{n}|\mu_{2},\Sigma}\right)\right]^{1-t_{n}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathrm{t}=\left(t_{1},\dots,t_{N}\right)^{T}.$
\end_inset

 Since both Gaussians do not contain 
\begin_inset Formula $\pi$
\end_inset

, it is easy to maximize w.r.t.
 
\begin_inset Formula $\pi$
\end_inset

 which yields
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\pi=\frac{N_{1}}{N_{1}+N_{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
w.r.t.
 
\begin_inset Formula $\mu_{1}$
\end_inset

 and 
\begin_inset Formula $\mu_{2}$
\end_inset

, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mu_{1} & =\frac{1}{N_{1}}\sum_{n=1}^{N}t_{n}x_{n}\\
\mu_{2} & =\frac{1}{N_{2}}\sum_{n=1}^{N}\left(1-t_{n}\right)x_{n}
\end{align*}

\end_inset

 which are the arithmetic mean of their respective class.
\end_layout

\begin_layout Standard
w.r.t.
 
\begin_inset Formula 
\[
\Sigma=S=\frac{N_{1}}{N}S_{1}+\frac{N_{2}}{N}S_{2}
\]

\end_inset

 where 
\begin_inset Formula $S_{1}$
\end_inset

 and 
\begin_inset Formula $S_{2}$
\end_inset

 are the MAL estimate of covariance matrix of the corresponding class.
\end_layout

\begin_layout Subsection
The Naive Bayes' Classifier
\end_layout

\begin_layout Standard
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Probabilistic_model
\end_layout

\end_inset


\end_layout

\begin_layout Section
4.3 Probabilistic Discriminative Models
\end_layout

\begin_layout Standard
The indirect approach to finding the parameters of a generalized linear
 model, by fitting class-conditional densities and class priors separately
 and then applying Bayes' theorem, represents an example of 
\emph on

\begin_inset CommandInset href
LatexCommand href
name "generative modelling"
target "https://en.wikipedia.org/wiki/Generative_model"
literal "false"

\end_inset

.

\emph default
 In the direct approach, we are maximizing a likelihood function defined
 through the conditional distribution 
\begin_inset Formula $p\left(C_{k}|x\right)$
\end_inset

, which is a form of 
\emph on

\begin_inset CommandInset href
LatexCommand href
name "discriminative"
target "https://en.wikipedia.org/wiki/Discriminative_model"
literal "false"

\end_inset

 
\emph default
training.
\end_layout

\begin_layout Subsection
4.3.2 Logistic regression
\end_layout

\begin_layout Standard
This is a model for classification rather than regression.
\end_layout

\begin_layout Standard
The posterior probability of class 
\begin_inset Formula $C_{1}$
\end_inset

 
\begin_inset Formula 
\[
p\left(C_{1}|\phi\right)=y\left(\phi\right)=\sigma\left(w^{T}\phi\right)
\]

\end_inset

 with 
\begin_inset Formula $p\left(C_{2}|\phi\right)=1-p\left(C_{1}|\phi\right)$
\end_inset

.
\end_layout

\begin_layout Standard
For an 
\begin_inset Formula $M$
\end_inset

-dimensional feature space 
\begin_inset Formula $\phi$
\end_inset

, this model has 
\begin_inset Formula $M$
\end_inset

 adjustable parameters, in contrast with Gaussian MAL, which has a total
 of 
\begin_inset Formula $M\left(M+5\right)/2+1$
\end_inset

.
 For large values of 
\begin_inset Formula $M$
\end_inset

, there is a clear advantage in working with the logistic regression model
 directly.
\end_layout

\begin_layout Paragraph
MAL
\end_layout

\begin_layout Standard
For a data set 
\begin_inset Formula $\left\{ \phi_{n},t_{n}\right\} $
\end_inset

, where 
\begin_inset Formula $t_{n}\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $\phi_{n}=\phi\left(x_{n}\right)$
\end_inset

, with 
\begin_inset Formula $n=1,\dots,N$
\end_inset

.
 The likelihood function
\begin_inset Formula 
\[
p\left(\mathrm{t}|w\right)=\prod_{n=1}^{N}y_{n}^{t_{n}}(1-y_{n})^{1-t_{n}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathrm{t}=\left(t_{1},\dots,t_{N}\right)^{T}$
\end_inset

 and 
\begin_inset Formula $y_{n}=p\left(C_{1}|\phi_{n}\right)$
\end_inset


\end_layout

\begin_layout Standard
Our goal is to minimize the 
\emph on
cross-entropy 
\emph default
error function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E\left(w\right)=-\ln p\left(t|w\right)=-\sum_{n=1}^{N}\left\{ t_{n}\ln y_{n}+\left(1-t_{n}\right)\ln\left(1-y_{n}\right)\right\} 
\]

\end_inset

 where 
\begin_inset Formula $y_{n}=\sigma\left(w^{T}\phi_{n}\right)$
\end_inset

.
 Taking the gradient of 
\begin_inset Formula $E\left(w\right)$
\end_inset

 w.r.t.
 
\begin_inset Formula $w$
\end_inset

, we obtain 
\begin_inset Formula 
\[
\triangledown E\left(w\right)=\sum_{n=1}^{N}\left(y_{n}-t_{n}\right)\phi_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
This takes the same form as the gradient of the sum-of-squares error function
 for the linear regression model.
\end_layout

\begin_layout Standard
This gradient expression gives a sequential algorithm in which patterns
 are presented, i.e.
 
\begin_inset Formula 
\[
w^{\left(k\right)}=w^{\left(k-1\right)}-\eta\left(k\right)\sum_{n=1}^{N}\left[y_{n}-t_{n}\right]\phi_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
MAL can exhibit severe overfitting for data sets that are linearly separable.
 When 
\begin_inset Formula $\sigma=0.5$
\end_inset

, the logistic sigmoid function becomes infinitely steep in feature space,
 corresponding to a Heaviside step function, so that every training point
 from each class 
\begin_inset Formula $k$
\end_inset

 is assigned a posterior probability 
\begin_inset Formula $p\left(C_{k}|x\right)=1$
\end_inset

.
 There is a continuum of such solutions because any separating hyperplane
 will give rise to the same posterior probabilities at the training data
 points, in which case MAL provides no way to favor one over another, depending
 on the choice of optimization algorithm and the parameter initialization.The
 singularity can be avoided by inclusion of a prior and finding a MAP solution
 for 
\begin_inset Formula $w$
\end_inset

, or equivalently by adding a regularization term to the error function.
 
\end_layout

\begin_layout Chapter
6.
 Kernel Method
\end_layout

\begin_layout Standard
There is a class of pattern recognition techniques in which the training
 data points, or a subset of them are kept and used also during the prediction
 phase such as KNN or the Parzen probability density model.
 These are 
\emph on
memory-based methods, 
\emph default
typically requiring a a metric that measures the similarity of any two vectors
 in input spaces, fast at training but slow at making predictions.
 Many linear parametric models can be recast into an equivalent 
\begin_inset Quotes eld
\end_inset

dual representation
\begin_inset Quotes erd
\end_inset

 in which the predictions are also based on linear combinations of a kernel
 function evaluated at the training data points.
\end_layout

\begin_layout Paragraph
The Concept of Kernel Function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
k(x,x')=\phi(x)^{T}\phi(x')
\]

\end_inset


\end_layout

\begin_layout Standard
which is a symmetric function 
\begin_inset Formula $k(x,x')=k(x',x)$
\end_inset


\end_layout

\begin_layout Standard
This allows us to build interesting extensions of many well-known algorithms
 by making use of the 
\emph on
kernel trick
\emph default
(
\emph on
kernel substituion
\emph default
), i.e.
 if we have an algorithm formulated in such a way that the input vector
 
\begin_inset Formula $x$
\end_inset

 enters only in the form of 
\begin_inset CommandInset href
LatexCommand href
name "scalar products"
target "https://en.wikipedia.org/wiki/Dot_product"
literal "false"

\end_inset

, then scalar product can be replaced with some other choice of kernel.
\end_layout

\begin_layout Standard
Application Examples:
\end_layout

\begin_layout Enumerate
Nonlinear variant of PCA 
\end_layout

\begin_layout Enumerate
nearest -neighbor classifiers and the kernel Fisher discriminant 
\end_layout

\begin_layout Standard

\emph on
Stationary kernel
\emph default
: 
\begin_inset Formula $k(x,x')=k(x-x')$
\end_inset

, which is invariant to translations in input space
\end_layout

\begin_layout Standard

\emph on
homogeneous kernel
\emph default
(a.k.a 
\emph on
radial basis function
\emph default
): 
\begin_inset Formula $k(x,x')=k(\parallel x-x'\parallel)$
\end_inset

 , which depend only on magnitude of the distance.
\end_layout

\begin_layout Section
6.1 Dual Representations
\end_layout

\begin_layout Standard
Many linear models for regression and classification can be reformulated
 in terms of a dual representation in which the kernel function arises naturally.
\end_layout

\begin_layout Standard
Consider the case of 
\emph on
weight decay, 
\emph default
a regularized sum-of-squares error function
\begin_inset Formula 
\[
J\left(w\right)=\frac{1}{2}\sum_{n=1}^{N}\left\{ w^{T}\phi\left(x_{n}\right)-t_{n}\right\} ^{2}+\frac{\lambda}{2}w^{T}w
\]

\end_inset


\end_layout

\begin_layout Standard
Take the gradient w.r.t.
 
\begin_inset Formula $w$
\end_inset

, but do not compute its closed from 
\begin_inset Formula 
\[
w=\Phi^{T}a
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\Phi$
\end_inset

 is the design matrix, 
\begin_inset Formula $a=-\frac{1}{\lambda}\left\{ w^{T}\phi\left(x_{n}\right)-t_{n}\right\} $
\end_inset

.
 
\end_layout

\begin_layout Standard
Substitute them back into 
\begin_inset Formula $J\left(w\right)$
\end_inset

, which gives a 
\emph on
dual representation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J\left(a\right)=\frac{1}{2}a^{T}\Phi\Phi^{T}\Phi\Phi^{T}a-a^{T}\Phi\Phi^{T}t+\frac{1}{2}t^{T}t+\frac{\lambda}{2}a^{T}\Phi\Phi^{T}a
\]

\end_inset

 define the symmetric 
\emph on
Gram 
\emph default
matrix 
\begin_inset Formula $K=\Phi\Phi^{T}$
\end_inset

whose element 
\begin_inset Formula 
\[
K_{n,m}=\phi\left(x_{n}\right)^{T}\phi\left(x_{m}\right)=k\left(x_{n},x_{m}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
substituting it back into 
\begin_inset Formula $J\left(a\right)$
\end_inset

, taking the gradient w.r.t.
 
\begin_inset Formula $a$
\end_inset

 and setting it to zero, the solution for 
\begin_inset Formula $a$
\end_inset

 is 
\begin_inset Formula 
\[
a=\left(K+\lambda I_{N}\right)^{-1}t
\]

\end_inset


\end_layout

\begin_layout Standard
The linear regression model becomes
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y\left(x\right)=w^{T}\phi\left(x\right)=a^{T}\Phi\phi\left(x\right)=\begin{bmatrix}k\left(x_{1},x\right)\\
\vdots\\
k\left(x_{n},x\right)\\
\vdots\\
k\left(x_{N},x\right)
\end{bmatrix}^{T}\left(K+\lambda I_{N}\right)^{-1}t
\]

\end_inset


\end_layout

\begin_layout Standard
Note in this equation, there is a kernel function 
\begin_inset Formula $K$
\end_inset

 to be found, and training data 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $x_{n}$
\end_inset

(lying inside 
\begin_inset Formula $k\left(x_{n},x\right)$
\end_inset

 and 
\begin_inset Formula $K$
\end_inset

) still in use, although not very useful here due to the huge dimension
 of 
\begin_inset Formula $K$
\end_inset

.
 The advantage is that we can therefore work directly in terms of kernels
 and avoid the explicit introduction of the feature vector 
\begin_inset Formula $\phi(x)$
\end_inset

 , which allows us implicitly to use feature spaces of high, even infinite
 dimensionality.
 The exisitence of a dual representation based on the Gram matrix is a property
 of many linear models, including the perceptron.
\end_layout

\begin_layout Section
6.2 Contructing Kernels
\end_layout

\begin_layout Standard
A kernel can be built by using either 
\begin_inset Formula $\phi(x)$
\end_inset

 or directly.
\end_layout

\begin_layout Standard
To simply test whether a function is a valid kernel, we have:
\end_layout

\begin_layout Standard
Give a set 
\begin_inset Formula $\{x_{n}\}$
\end_inset

 and a kernel-to-be function 
\begin_inset Formula $k(x,x')$
\end_inset

 is valid if and only if the Gram matrix 
\begin_inset Formula $K$
\end_inset

, whose elements are given by 
\begin_inset Formula $k(x_{n},x_{m})$
\end_inset

 is positive semidefinite.
\end_layout

\begin_layout Standard
Techiniques for Contructing New Kernels by valid kernels See P.
 296 PRML by Bishop
\end_layout

\begin_layout Standard
E.g.
\end_layout

\begin_layout Enumerate
Gaussian Kenel: 
\begin_inset Formula $k\left(x,x'\right)=\exp\left(-\left\Vert x-x'\right\Vert ^{2}/2\sigma^{2}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Polynomial kernel: 
\begin_inset Formula $k(x,x')=(x^{T}x'+c)^{M}$
\end_inset

 
\end_layout

\begin_layout Standard
An important contribution to arise from the kernel viewpoint has been the
 extension to inputs that are symbolic, rather than simply vectors of real
 numbers.
 Kernel functions can be defined over objects as diverse as graphs, sets,
 strings, and text documents.
\end_layout

\begin_layout Paragraph
Building Kernels from a probabilistic generative model
\end_layout

\begin_layout Standard
Consider a set of probability densities 
\begin_inset Formula $\left\{ p\left(x|i\right)\right\} $
\end_inset

 , 
\begin_inset Formula $k_{i}\left(x,x'\right)=p\left(x|i\right)p\left(x'|i\right)$
\end_inset

 is a kernel and 
\begin_inset Formula 
\[
k\left(x,x'\right)=\sum_{i}p\left(i\right)k_{i}\left(x,x'\right)=\sum_{i}p\left(x|i\right)p\left(x'|i\right)p\left(i\right)
\]

\end_inset


\end_layout

\begin_layout Standard
is also a kernel, equivalent to a mixture distribution in which the components
 factorize, with the index 
\begin_inset Formula $i$
\end_inset

 playing the role of a 
\emph on
latent variable.
\end_layout

\begin_layout Standard
Taking the limit of an infinite sum, 
\begin_inset Formula 
\[
k\left(x,x'\right)=\int p\left(x|z\right)p\left(x'|z\right)p\left(z\right)dz
\]

\end_inset


\end_layout

\begin_layout Standard
is also a kernel.
\end_layout

\begin_layout Enumerate
hidden Markov model 
\end_layout

\begin_layout Enumerate
Fisher kernel 
\end_layout

\begin_layout Enumerate
Sigmoidal kernel 
\begin_inset Formula 
\[
k\left(x,x'\right)=\tanh\left(ax^{T}x'+b\right)
\]

\end_inset

whose Gram matrix in general is not p.s.d but useful in practice.
\end_layout

\begin_layout Chapter
7.
 Sparse Kernel marchines
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "Lagrange Multiplier"
target "https://en.wikipedia.org/wiki/Lagrange_multiplier#Single_constraint"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "KKT Conditions"
target "https://en.wikipedia.org/wiki/KarushKuhnTucker_conditions"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
One of the significant limitations of many such algorithms is that the kernel
 function 
\begin_inset Formula $k(x_{n},x_{m})$
\end_inset

 must be evaluated for all possible pairs 
\begin_inset Formula $x_{m}$
\end_inset

 and 
\begin_inset Formula $x_{n}$
\end_inset

 of training points, which can be computationally infeasible during training
 and can lead to excessive computation times when making predictions for
 new data points.
\end_layout

\begin_layout Standard
Here we discuss about kernel-based algorithms that have 
\emph on
sparse solutions, 
\emph default
so that predictions for new inputs depend only on the kernel function evaluated
 at a 
\series bold
subset 
\series default
of the training data points.
\end_layout

\begin_layout Section
7.1 Maximum Margin Classifiers (SVM)
\end_layout

\begin_layout Standard
The SVM is a decision machine and so does not provide posterior probabilities.
\end_layout

\begin_layout Standard

\emph on
Margin
\emph default
: the smallest distance between the decision boundary and any of the samples
 
\end_layout

\begin_layout Standard
Consider the two-class classification problem using linear models of the
 form
\begin_inset Formula 
\[
y\left(x\right)=w^{T}\phi\left(x\right)+b
\]

\end_inset


\end_layout

\begin_layout Standard
The training data set comprises 
\begin_inset Formula $N$
\end_inset

 input vectors 
\begin_inset Formula $x_{1},\dots,x_{N}$
\end_inset

, with the corresponding target values 
\begin_inset Formula $t_{n}\in\left\{ -1,1\right\} $
\end_inset

, and new data points are classified according the 
\series bold
sign 
\series default
of 
\begin_inset Formula $y\left(x_{n}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Given a decision boundary hyperplane 
\begin_inset Formula $y(x)=0$
\end_inset

, the margin is given by 
\begin_inset Formula $|y(x)|/\left\Vert w\right\Vert $
\end_inset

, Futhermore,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
t_{n}y(x_{n})=\left\Vert y(x_{n})\right\Vert >0\qquad\text{for\ all\ correctly\ classified\ n}
\]

\end_inset

 thus we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{Distance}=\dfrac{t_{n}y(x_{n})}{\left\Vert w\right\Vert }=\dfrac{t_{n}(w^{T}\phi(x_{n})+b)}{\left\Vert w\right\Vert }
\]

\end_inset


\end_layout

\begin_layout Standard
And our optimization problem becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{w,b}{\text{arg max}}\text{ Margin}=\underset{w,b}{\text{arg \text{max}}}\left\{ \underset{n}{\text{min}}\left[\dfrac{1}{\left\Vert w\right\Vert }t_{n}\left(w^{T}\phi(x_{n})+b\right)\right]\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
Still not concise enough, take out 
\begin_inset Formula $1/\left\Vert w\right\Vert $
\end_inset

 outside the optimization over 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{Suppose}\ y=\kappa,\text{\ then\ we\ can\ let\ }b=b/\kappa,\ w=w/\kappa,\ \text{then}\ y=1\text{ for the point closest to the surface}
\]

\end_inset


\end_layout

\begin_layout Standard
The distance is still the same
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{Margin}=\dfrac{1}{\left\Vert w\right\Vert }
\]

\end_inset


\end_layout

\begin_layout Standard
And all points satisfy
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
t_{n}(w^{T}\phi(x_{n})+b)\geq1\quad n=1,...,N
\]

\end_inset


\end_layout

\begin_layout Standard
Thus we have a quadratic programming problem
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{w,b}{\text{arg}\text{min}}\dfrac{1}{2}\left\Vert w\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Constrained by the inequalities above
\end_layout

\begin_layout Standard
Lagrange function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(w,b,a)=\dfrac{1}{2}\left\Vert w\right\Vert ^{2}-\sum\limits _{n=1}^{N}a_{n}\left[t_{n}(w^{T}\phi(x_{n})+b)-1\right]
\]

\end_inset


\end_layout

\begin_layout Standard
By introducing Lagrange multipliers and using KKT-Conditions, minimizing
 w.r.t.
 
\begin_inset Formula $w$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\triangledown_{w}L=0:w & =\sum\limits _{n=1}^{N}a_{n}t_{n}\phi(x_{n})\\
\triangledown_{b}L=0:\sum\limits _{n=1}^{N}a_{n}t_{n} & =0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Substitute the result above back into the Lagrange function, the 
\begin_inset CommandInset href
LatexCommand href
name "dual representation"
target "https://en.wikipedia.org/wiki/Duality_(optimization)#The_strong_Lagrangian_principle:_Lagrange_duality"
literal "false"

\end_inset

 is obtained
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\underset{a}{\text{arg}\text{max}}L_{D}(a) & =\sum\limits _{n=1}^{N}a_{n}-\dfrac{1}{2}\sum\limits _{n=1}^{N}\sum\limits _{m=1}^{N}a_{m}a_{n}t_{m}t_{n}k(x_{n},x_{m})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula 
\[
k(x_{n},x_{m})=\phi(x_{n})^{T}\phi(x_{m}),\ \text{a\ kernel\ function}
\]

\end_inset


\end_layout

\begin_layout Standard
Constrained by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
a_{n} & \geq0,\qquad n=1,...,N\\
\sum\limits _{n=1}^{N}a_{n}t_{n} & =0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
our classifier becomes
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y(x)=\sum\limits _{n=1}^{N}a_{n}t_{n}k(x,x_{n})+b
\]

\end_inset


\end_layout

\begin_layout Standard
Now here we have a new problem, to solve this optimization problem
\end_layout

\begin_layout Subsection*
SMO Algorithm
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "The Proof of SMO"
target "http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
From our very beginning of the optimzation problem, we have 
\emph on
dual feasibility
\emph default
 and 
\emph on
complementary slackness
\emph default
 as well as 
\emph on
primal feasibility
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
a_{n} & \geq0\\
a_{n}\left[t_{n}y(x_{n})-1\right] & =0\\
t_{n}y(x_{n})-1 & \geq0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note is from Problem 
\begin_inset Formula $(7)$
\end_inset


\end_layout

\begin_layout Standard
Either 
\begin_inset Formula $a_{n}=0$
\end_inset

 or 
\begin_inset Formula $t_{n}y(x_{n})-1=0$
\end_inset

, the former will drive the corresponding data point out of our optimization
 problem, and the latter makes the point so-called 
\emph on
support vectors
\emph default
, which are the only points retained after training.
\end_layout

\begin_layout Standard
Combine the complementary slackness and , by multiplying 
\begin_inset Formula $t_{n}$
\end_inset

 at both sides, we can solve 
\begin_inset Formula $b$
\end_inset

, but we would prefer to average over all support vectors and that gives
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
b & =\dfrac{1}{N_{S}}\sum\limits _{n\in\text{SV}}(t_{n}-\sum\limits _{m\in\text{SV}}a_{m}t_{m}k(x_{n},x_{m}))\\
\text{SV} & =\text{the\ set\ of\ Support\ Vectors}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
SVM (Maximum margin classifier) in terms of the minization of an error function
\begin_inset Formula 
\[
\sum_{n=1}^{N}E_{\infty}\left(y\left(x_{n}\right)t_{n}-1\right)+\lambda\left\Vert w\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $E_{\infty}\left(z\right)$
\end_inset

 is a function that is zero if 
\begin_inset Formula $z\geq0$
\end_inset

 and 
\begin_inset Formula $\infty$
\end_inset

 otherwise and ensures that the constraints 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $t_{n}y(x_{n})-1\geq0$
\end_inset

 are satisfied.
\end_layout

\begin_layout Subsection
7.1.1 Overlapping class distributions
\end_layout

\begin_layout Standard
The error function form of hard SVM gives infinite error if a data point
 was misclassified.
 We modify this approach so that data points are allowed to be on the wrong
 side of the margin boundary, but with a penalty that increases with the
 distance from the boundary.
\end_layout

\begin_layout Standard

\emph on
Slack variables 
\begin_inset Formula $\left\{ \xi_{n}\right\} $
\end_inset

 
\emph default
for 
\begin_inset Formula $n=1,\dots,N$
\end_inset

, defined by
\begin_inset Formula 
\begin{align*}
\xi_{n} & =0\text{ for data points on or inside the correct margin boundary.}\\
\xi_{n} & =\left|t_{n}-y\left(x_{n}\right)\right|,\text{ for other points}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
thus
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\xi_{n}=0$
\end_inset

 for points on or inside the correct margin boundary;
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\xi_{n}=1$
\end_inset

 if the point is on the decision surface; 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $0<\xi_{n}<1$
\end_inset

: inside the margin but not misclassified;
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\xi_{n}>1$
\end_inset

 are misclassified (
\begin_inset Formula $y\left(x_{n}\right)<0,t_{n}=1$
\end_inset

 or 
\begin_inset Formula $y\left(x_{n}\right)>0,t_{n}=-1$
\end_inset

).
\end_layout

\begin_layout Standard
which gives rise to a 
\emph on
soft margin.

\emph default
 The penalty for misclassification increases linearly with 
\begin_inset Formula $\xi$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{w,b,\xi_{n}}{\text{arg min}}C\sum_{n=1}^{N}\xi_{n}+\frac{1}{2}\left\Vert w\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
subject to
\begin_inset Formula 
\begin{align*}
t_{n}y\left(x_{n}\right) & \geq1-\xi_{n}\text{ for }n=1,\dots,N\\
\xi_{n} & \geq0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $C>0$
\end_inset

 controls the trade-ff between minimizing training errors and controlling
 mdoel complexity.
 In the limit 
\begin_inset Formula $C\rightarrow\infty$
\end_inset

, this becomes the hard margin.
\end_layout

\begin_layout Standard
there exist KKT multipliers 
\begin_inset Formula $a_{n}$
\end_inset

 and 
\begin_inset Formula $\mu_{n}$
\end_inset

 s.t.
\end_layout

\begin_layout Standard
1.
 Primary feasibility
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
1-\xi_{n}^{*}-t_{n}y\left(x_{n}\right) & \leq0\\
-\xi_{n}^{*} & \leq0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
2.
 Dual feasibility
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
a_{n} & \geq0\\
\mu_{n} & \geq0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
3.
 Complementary slackness
\begin_inset Formula 
\begin{align*}
a_{n}\left(1-\xi_{n}^{*}-t_{n}y\left(x_{n}\right)\right) & =0\\
\mu_{n}\xi_{n}^{*} & =0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
4.
 Stationarity
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{w.r.t }w & :w^{*}=\sum_{n=1}^{N}a_{n}t_{n}\phi\left(x_{n}\right)\\
\text{w.r.t }b & \sum_{n=1}^{N}a_{n}t_{n}=0\\
\text{w.r.t }\xi_{n} & C=a_{n}+\mu_{n}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Use the conditions above to eliminate 
\begin_inset Formula $w$
\end_inset

, 
\begin_inset Formula $b$
\end_inset

 and 
\begin_inset Formula $\xi_{n}$
\end_inset

in the original Lagrangian, yielding
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\tilde{L}\left(a\right)=\sum_{n=1}^{N}a_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}a_{m}a_{n}t_{m}t_{n}k\left(x_{n},x_{m}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
subject to 
\begin_inset Formula 
\begin{align*}
0\leq a_{n} & \leq C\\
\sum_{n=1}^{N}a_{n}t_{n} & =0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
for 
\begin_inset Formula $n=1,\dots,N$
\end_inset

 to be maximized.
\end_layout

\begin_layout Standard
Still, our classifier becomes
\begin_inset Formula 
\[
y(x)=\sum\limits _{n=1}^{N}a_{n}t_{n}k(x,x_{n})+b
\]

\end_inset


\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $a_{n}=0$
\end_inset

, this point does not contribute to the predictive model, inside the correct
 margin.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $a_{n}<C$
\end_inset

, then 
\begin_inset Formula $\mu_{n}>0\rightarrow\xi_{n}=0$
\end_inset

 , this point is on the correct margin.
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $a_{n}=C$
\end_inset

, then either correctly classified if 
\begin_inset Formula $\xi_{n}\leq1$
\end_inset

 or misclassified if 
\begin_inset Formula $\xi_{n}>1$
\end_inset


\end_layout

\begin_layout Standard
Again, 
\begin_inset Formula 
\[
b=\frac{1}{N_{M}}\sum_{n\in M}\left(t_{n}-\sum_{m\in S}a_{m}t_{m}k\left(x_{n},x_{m}\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $M=\left\{ \text{data points }i|0<a_{i}<C\right\} $
\end_inset

 and S is the set of support vectors.
\end_layout

\begin_layout Standard
An alternative equivalent formulation of the SVM, known as 
\begin_inset Formula $\nu$
\end_inset

-SVM, has been proposed.
 This approach has the advantage that the parameter 
\begin_inset Formula $\nu$
\end_inset

, can be interpreted as both an upper bound on the fraction of 
\emph on
margin errors, 
\emph default
and a lower bound on the fraction of support vectors.
\end_layout

\begin_layout Standard
On optimize the quadratic programming problem ,there are 
\emph on
chunking, decomposition methods,
\emph default
 and the most popular approach, 
\emph on
sequential minimal optimization 
\emph default
(
\emph on
SMO).
\end_layout

\begin_layout Subsection
7.1.2 Relation to logistic regression
\end_layout

\begin_layout Chapter
9.
 Mixture Models and EM
\end_layout

\begin_layout Standard
The introduction of latent variables allows complicated distribution to
 be formed from simpler components.
\end_layout

\begin_layout Standard
Gaussian mixture models are widely used in data mining, pattern recognition,
 machine learning and statistical analysis.
 Their paramerters are usually determined by MAL, typically using the EM
 algorithm.
\end_layout

\begin_layout Section
9.1 K-means Clustering
\end_layout

\begin_layout Standard
Given a data set 
\begin_inset Formula $\left\{ x_{1},\dots,x_{N}\right\} $
\end_inset

 of 
\begin_inset Formula $N$
\end_inset

 observations of a random 
\begin_inset Formula $D$
\end_inset

-dimensional Euclidean variable 
\begin_inset Formula $x$
\end_inset

.
 We introduce a binary indicator variables 
\begin_inset Formula $r_{nk}\in\left\{ 0,1\right\} ,$
\end_inset

for 
\begin_inset Formula $k=1,\dots,K.$
\end_inset


\begin_inset Formula 
\[
r_{nk}=\begin{cases}
1 & x_{n}\in C_{k}\\
0 & x_{n}\notin C_{k}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
i.e.
 the 1-of-K coding scheme.
 
\end_layout

\begin_layout Standard
Define an objective function called 
\emph on
distortion measure
\emph default

\begin_inset Formula 
\[
J=\sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\left\Vert x_{n}-\mu_{k}\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mu_{k}$
\end_inset

 represents the center of cluster 
\begin_inset Formula $K$
\end_inset

 (not necessarily the mean).
\end_layout

\begin_layout Standard
Our goal is to find values for the 
\begin_inset Formula $\left\{ r_{nk}\right\} $
\end_inset

 and the 
\begin_inset Formula $\left\{ \mu_{k}\right\} $
\end_inset

 so as to minimize 
\begin_inset Formula $J$
\end_inset

.
 This is done by the EM algorithm.
\end_layout

\begin_layout Standard
First initialize 
\begin_inset Formula $\mu_{k}$
\end_inset

, minimize 
\begin_inset Formula $J$
\end_inset

 w.r.t.
 
\begin_inset Formula $r_{nk}$
\end_inset

, keeping 
\begin_inset Formula $\mu_{k}$
\end_inset

 fixed (Expectation), then minimize 
\begin_inset Formula $J$
\end_inset

 w.r.t.
 
\begin_inset Formula $\mu_{k}$
\end_inset

, keeping 
\begin_inset Formula $r_{nk}$
\end_inset

 fixed (maximization), until convergence.
\end_layout

\begin_layout Paragraph
Expectation
\end_layout

\begin_layout Standard
\begin_inset Formula $J$
\end_inset

 is a linear function of 
\begin_inset Formula $r_{nk}$
\end_inset

, giving a closed form solution.
 The terms involving different 
\begin_inset Formula $n$
\end_inset

 are independent and we can optimize for each 
\begin_inset Formula $n$
\end_inset

 separately, which is obviously done by choosing
\begin_inset Formula 
\begin{align*}
r_{nk} & =\begin{cases}
1 & \text{if \ensuremath{k}=\text{arg min}_{j}\left\Vert x_{n}-\mu_{j}\right\Vert ^{2}}\\
0 & \text{otherwise}
\end{cases}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
Maximization
\end_layout

\begin_layout Standard
The objective 
\begin_inset Formula $J$
\end_inset

 is a quadratic function of 
\begin_inset Formula $\mu_{k}$
\end_inset

, which can be minimized by setting its derivative w.r.t.
 
\begin_inset Formula $\mu_{k}$
\end_inset

 to zero 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mu_{k}=\frac{\sum_{n}r_{nk}x_{n}}{\sum_{n}r_{nk}}
\]

\end_inset


\end_layout

\begin_layout Standard
Because each phase reduces the value of 
\begin_inset Formula $J$
\end_inset

, convergence of the algorithm is assured.
 It is also worth noting that the K-means algorithm itself is often used
 to initialize the parameters in a Gaussian mixture model before applying
 the EM algorithm.
 A direct implementation of the K-means algorithm as discussed here can
 be relatively slow, because in each E step it is necessary to compute the
 Euclidean dis- tance between every prototype vector and every data point.
 Various schemes have been proposed for speeding up the K-means algorithm.
\end_layout

\begin_layout Paragraph
Online stochastic algorithm
\end_layout

\begin_layout Standard
Apply the Robbins-Monro procedure
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mu_{k}^{\text{new }}=\mu_{k}^{\text{old }}+\eta_{n}\left(x_{n}-\mu_{k}^{\text{old }}\right)
\]

\end_inset


\end_layout

\begin_layout Paragraph
A generalization of the similarity measure
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\tilde{J}=\sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\mathcal{V}\left(x_{n},\mu_{k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
which gives rise to 
\emph on
K-medoids 
\emph default
algorithm.
\end_layout

\begin_layout Standard
One notable feature of the K-means algorithm is that at each iteration,
 every data point is assigned uniquely to one, and only one, of the clusters.
 Whereas some data points will be much closer to a particular centre 
\begin_inset Formula $\mu_{k}$
\end_inset

 than to any other centre, there may be other data points that lie roughly
 midway between cluster centres.
\end_layout

\begin_layout Section
9.2 Mixtures of Gaussians
\end_layout

\begin_layout Standard
A linear superposition of Gaussians
\begin_inset Formula 
\[
p\left(x\right)=\sum_{k=1}^{N}\pi_{k}N\left(x|\mu_{k},\Sigma_{k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Define a 
\begin_inset Formula $K$
\end_inset

-dimensional binary random variable 
\begin_inset Formula $\text{z}$
\end_inset

 haing a 1-of-
\begin_inset Formula $K$
\end_inset

 representation satisfying 
\begin_inset Formula $z_{k}\in\left\{ 0,1\right\} $
\end_inset

 and 
\begin_inset Formula $\sum_{k}z_{k}=1$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p\left(z_{k}=1\right)=\pi_{k}
\]

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $0\leq\pi_{k}\leq1$
\end_inset

 and 
\begin_inset Formula $\sum_{k=1}^{K}\pi_{k}=1$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p\left(z\right)=\prod_{k=1}^{K}\pi_{k}^{z_{k}}
\]

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $p\left(x|z_{k}=1\right)=N\left(x|_{k},\Sigma_{k}\right)$
\end_inset

,
\begin_inset Formula 
\[
p\left(\mathrm{x}|\mathrm{z}\right)=\prod_{k=1}^{K}N\left(\mathrm{x}|\mu_{k},\Sigma_{k}\right)^{z_{k}}
\]

\end_inset


\end_layout

\begin_layout Standard
thus
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p\left(x\right)=\sum_{z}p\left(z\right)p\left(x|z\right)=\sum_{k=1}^{K}\pi_{k}N\left(x|\mu_{k},\Sigma_{k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
It follows that every observed data point 
\begin_inset Formula $\mathrm{x_{n}}$
\end_inset

 has a corresponding latent variable 
\begin_inset Formula $\mathrm{z_{n}}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\gamma\left(z_{k}\right)\equiv p\left(z_{k}=1|x\right) & =\frac{p\left(z_{k}=1\right)p\left(\mathrm{x}|z_{k}=1\right)}{\sum_{j=1}^{K}p\left(z_{j}=1\right)p\left(\mathrm{x}|z_{j}=1\right)}\\
 & =\frac{\pi_{k}N\left(x|\mu_{k},\Sigma_{k}\right)}{\sum_{j=1}^{K}\pi_{j}N\left(x|\mu_{j},\Sigma_{j}\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
viewed as the 
\emph on
responsibility 
\emph default
that component 
\begin_inset Formula $k$
\end_inset

 takes for explaining the observation 
\begin_inset Formula $\mathrm{x}$
\end_inset

.
\end_layout

\begin_layout Subsection
9.2.1 Maximum likelihood
\end_layout

\begin_layout Standard
Data matrx 
\begin_inset Formula $\mathrm{X}^{N\times D}$
\end_inset

 and latent variable matrix 
\begin_inset Formula $\mathrm{Z}^{N\times D}$
\end_inset


\end_layout

\begin_layout Standard
If we assume that the data points are drawn independently from the distribution,
 then we can express the Gaussian mixture model for this i.i.d.
 data set, the log-likelihood function is given by
\begin_inset Formula 
\[
\ln p\left(\mathrm{X}|\pi,\mu,\Sigma\right)=\sum_{n=1}^{N}\ln\left\{ \sum_{k=1}^{K}\pi_{k}N\left(x_{n}|\mu_{k},\Sigma_{k}\right)\right\} 
\]

\end_inset


\end_layout

\begin_layout Subsection
9.2.2 EM for Gaussian mixtures
\end_layout

\begin_layout Standard
An elegant and powerful method for finding maximum likelihood solutions
 for models with latent variables is called the 
\emph on
expectation-maximization 
\emph default
algorithm.
 Setting the derivatives of the likelihood function above w.r.t.
 
\begin_inset Formula $\mu_{k}$
\end_inset

 to zero, we obtain
\begin_inset Formula 
\[
0=-\sum_{n=1}^{N}\underbrace{\frac{\pi_{k}N\left(x|\mu_{k},\Sigma_{k}\right)}{\sum_{j=1}^{K}\pi_{j}N\left(x|\mu_{j},\Sigma_{j}\right)}}_{\gamma\left(z_{nk}\right)}\Sigma_{k}\left(x_{n}-\mu_{k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Rearrange 
\begin_inset Formula 
\[
\mu_{k}=\frac{1}{N_{k}}\sum_{n=1}^{N}\gamma\left(z_{nk}\right)\mathrm{x}_{n}
\]

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula 
\[
N_{k}=\sum_{n=1}^{N}\gamma\left(z_{nk}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
w.r.t.
\begin_inset Formula 
\[
\Sigma_{k}=\frac{1}{N_{k}}\sum_{n=1}^{N}\gamma\left(z_{nk}\right)\left(x_{n}-\mu_{k}\right)\left(x_{n}-\mu_{k}\right)^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
Finally, we maximize the likelihood function subject to 
\begin_inset Formula $\sum_{k=1}^{K}\pi_{k}=1$
\end_inset

 w.r.t.
 
\begin_inset Formula $\pi_{k}$
\end_inset

 
\begin_inset Formula 
\[
\ln p\left(\mathrm{X}|\pi,\mu,\Sigma\right)+\lambda\left(\sum_{k=1}^{K}\pi_{k}-1\right)
\]

\end_inset


\end_layout

\begin_layout Standard
giving 
\begin_inset Formula $\lambda=-N$
\end_inset


\begin_inset Formula 
\[
\pi_{k}=\frac{N_{k}}{N}
\]

\end_inset


\end_layout

\begin_layout Standard
These results are not closed-form, but the EM algorithm do find a solution
 to the MAL problem.
 In the E step, we use these parameters to compute the posterior probabilities
 or responsibilities.
 In the M step, re-estimate the means, covariance and mixing coefficients
 using the above results.
\end_layout

\begin_layout Standard
Note that the EM algorithm takes many more iterations to reach (approximate)
 convergence compared with the K-means algorithm, and that each cycle requires
 significantly more computation.
 It is therefore common to run the K-means algo- rithm in order to find
 a suitable initialization for a Gaussian mixture model that is subsequently
 adapted using EM.
\end_layout

\end_body
\end_document
